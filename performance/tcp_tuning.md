These days, network speeds have gone up tremendously.
Before we had 10Mb/s to 1Gb/s network speeds.
Now 10Gb/s (10 Gigabits/sec) speeds are pretty normal. This puts a lot of pressure on the system CPU, because CPU's
now have to deal with a lot of TCP/IP network traffic and do stuff like

1) handling our of order packets
2) making resource intensive copies of data from kernel to user space
3) handling the protocol headers
4) handling interrupts.

Linux system are by default not configured for high network speed scenarios, so have to be fine tuned.
If you dont your system could be processing a lot of work just doing I/O rather than application processing.
So the overall system throughput can be limited.

This is a vast topic, so I'll try to explain as much as I can how we can fine tune some of the system settings
to increase 



1) Turn off the Nagle algorithm when your nodes are physically close together on a fast link and the data in the packet is not smaller than the TCP packet overhead. MS did a paper about this. A counter example to this is when you have a satellite connection in your link and a packet takes 800 milliseconds one way due to the distances involved. In that case, Nagle is a good thing.
2) Make sure you have the MTU size set correctly for your link and your machine to avoid fragmentation issues.
3) If you're on Windows and outgoing packets are small and the latency is high, you should also set the socket option SO_SNDBUF to 0 to avoid transmission delays.



```

	1. The design of the hardware, systems and protocols have to be considered together

	2. Develop protocols using UDP instead of TCP and implement simple ack-nak, resend logic at the application level

	3. Reduce the number of context switches (preferably to zero) for the process or thread that consumes and packetizes data off the wire

	4. Use the best selector for the OS (select, kqueue, epoll etc)

	5. Use good quality NICs and Switches with large amounts of on-board buffer (fifo)

	6. Use multiple NICs, specifically for down-stream and up-stream data flows

	7. Reduce the number of IRQs being generated by other devices or software (in short remove them if they are not required)

	8. Reduce the usage of mutexes and conditions. Instead where possible use Lock-Free programming techniques. Make use of the architecture's CAS capabilities. (Lock-Free containers)

	9. Consider single threaded over multi-threaded designs - context switches are very expensive.

	10. Understand and properly utilize your architecture's cache system (L1/L2, RAM etc)

	11. Prefer complete control over memory management, rather than delegating to Garbage Collectors

	12. Use good quality cables, keep the cables as short as possible, reduce the number of twists and curls


```

Others:
1: use userland networking stacks
2: service interrupts on the same socket as the handing code (shared cache)
3: prefer fixed length protocols, even if they are a little larger in bytes (quicker parsing)
4: ignore the network byte order convention and just use native ordering
5: never allocate in routines and object pool (esp. garbage collected languages)
6: try to prevent byte copying as much as possible (hard in TCP send)
7: use cut-through switching mode
8: hack networking stack to remove TCP slow start
9: advertise a huge TCP window (but don't use it) so the other side can have a lot of inflight packets at a time
10: turn off NIC coalescing, especially for send (packetize in the app stack if you need to)
11: prefer copper over optic
I can keep going, but that should get people thinking
One I don't agree with:
1: network cables are rarely an issue except when gone bad (there is an exception to this in terms of cable type)
